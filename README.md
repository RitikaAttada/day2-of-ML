# 🌱 Day 2 of My Machine Learning Journey

Welcome to **Day 2** of my Machine Learning learning journey! 🎯  
Today, I continued experimenting with regression models using **TensorFlow** and **Keras**.  
I explored how different layer depths and activation functions affect predictions in models trained on numerical patterns.

---

## 🧠 What I Did

- ✅ Trained multiple regression models using `Sequential` API
- ✅ Used activation functions like `relu` and `tanh`
- ✅ Tested deeper networks with multiple `Dense` layers
- ✅ Saved and loaded models using `.h5` files to avoid retraining
- ✅ Predicted values for inputs both within and beyond the training range

---

## 📚 Key Learnings

- Adding more layers can improve accuracy—but only up to a point
- `relu` is good but doesn't handle negative inputs
- `tanh` can work across negatives but may need tuning to avoid stuck loss
- You don’t always need a massive network for good performance
- Saving models avoids repeated training and saves time

---

## 🔖 Notes

I'm getting better at controlling how models behave and learning how to debug training issues.  
Excited to keep going and apply these concepts to more complex problems!

---

🗓️ **Date**: Day 2  
👩‍💻 **Author**: Ritika Attada  
🏁 **Status**: ✅ Completed  
📌 **Tools**: TensorFlow, Keras, NumPy, Python
