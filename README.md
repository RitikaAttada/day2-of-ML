# ğŸŒ± Day 2 of My Machine Learning Journey

Welcome to **Day 2** of my Machine Learning learning journey! ğŸ¯  
Today, I continued experimenting with regression models using **TensorFlow** and **Keras**.  
I explored how different layer depths and activation functions affect predictions in models trained on numerical patterns.

---

## ğŸ§  What I Did

- âœ… Trained multiple regression models using `Sequential` API
- âœ… Used activation functions like `relu` and `tanh`
- âœ… Tested deeper networks with multiple `Dense` layers
- âœ… Saved and loaded models using `.h5` files to avoid retraining
- âœ… Predicted values for inputs both within and beyond the training range

---

## ğŸ“š Key Learnings

- Adding more layers can improve accuracyâ€”but only up to a point
- `relu` is good but doesn't handle negative inputs
- `tanh` can work across negatives but may need tuning to avoid stuck loss
- You donâ€™t always need a massive network for good performance
- Saving models avoids repeated training and saves time

---

## ğŸ”– Notes

I'm getting better at controlling how models behave and learning how to debug training issues.  
Excited to keep going and apply these concepts to more complex problems!

---

ğŸ—“ï¸ **Date**: Day 2  
ğŸ‘©â€ğŸ’» **Author**: Ritika Attada  
ğŸ **Status**: âœ… Completed  
ğŸ“Œ **Tools**: TensorFlow, Keras, NumPy, Python
